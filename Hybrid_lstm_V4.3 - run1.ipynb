{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b7073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quantum\\anaconda3\\envs\\Quantum\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_qubits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_748\\3321083787.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpennylane\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mqml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"default.qubit.tf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwires\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_qubits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Run the model in classical CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Q1'\u001b[0m \u001b[1;31m#Used for saving training charakteristics file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mn_qubits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;31m#Number of qubits should be the same as number of features, max number = 25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_qubits' is not defined"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import tensorflow as tf\n",
    "model_name = 'Q1' #Used for saving training charakteristics file\n",
    "n_qubits = 2 #Number of qubits should be the same as number of features, max number = 25\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits) #Run the model in classical CPU\n",
    "blocks = 1 #Νumber of blocks (AngleEmbedding and StronglyEntanglingLayers is one block )\n",
    "layers = 1  #layers per block (multiple “layers” of StronglyEntanglingLayers per block )\n",
    "Epochs=50\n",
    "batch_size=32\n",
    "LR=0.0001 # Learning rate\n",
    "patience= 100 #For early stopping\n",
    "# Set the split ratios\n",
    "train_ratio = 0.75  # 70% for training\n",
    "val_ratio = 0.15  # 15% for validation\n",
    "ms = 'MS377'\n",
    "\n",
    "#%%\n",
    "df = pd.read_csv('traffic_mar23.csv',delimiter=';').iloc[:,1:]\n",
    "d1 = df[ms]\n",
    "from lstm_data_preparation import lstm_data\n",
    "x0, y0 = lstm_data(d1,ms,8)\n",
    "x0 = x0.reshape(x0.shape[0],x0.shape[1],1)\n",
    "#%%\n",
    "\n",
    "# Calculate the lengths of each set\n",
    "total_length = len(x0)\n",
    "train_length = int(total_length * train_ratio)\n",
    "val_length = int(total_length * val_ratio)\n",
    "test_length = total_length - train_length - val_length\n",
    "\n",
    "# Split the data\n",
    "x_train = x0[:train_length]\n",
    "y_train = y0[:train_length]\n",
    "x_val = x0[train_length:train_length+val_length]\n",
    "y_val = y0[train_length:train_length+val_length]\n",
    "x_test = x0[train_length+val_length:]\n",
    "y_test = y0[train_length+val_length:]\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"Training data shape:\", x_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", x_val.shape, y_val.shape)\n",
    "print(\"Testing data shape:\", x_test.shape, y_test.shape)\n",
    "\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Preprocess the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_scaled = x_train\n",
    "# Scale the validation dataset\n",
    "x_val_scaled = x_val\n",
    "\n",
    "# Scale the test dataset\n",
    "x_test_scaled = x_test\n",
    "\n",
    "# Scale the training dataset\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(-1, 1)).reshape(x_train.shape)\n",
    "\n",
    "# Scale the validation dataset\n",
    "x_val_scaled = scaler.transform(x_val.reshape(-1, 1)).reshape(x_val.shape)\n",
    "\n",
    "# Scale the test dataset\n",
    "x_test_scaled = scaler.transform(x_test.reshape(-1, 1)).reshape(x_test.shape)\n",
    "y_train =  scaler.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "y_val =  scaler.fit_transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "y_test =  scaler.fit_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "#%%\n",
    "'''from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "d1.iloc[:,:-1] = scaler.fit_transform(d1.iloc[:,:-1])\n",
    "corr=d1.corr()'''\n",
    "#%%\n",
    "\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "\n",
    "# Define quantum node\n",
    "# Added below to ensure the QNode is JIT compiled\n",
    "@tf.function\n",
    "@qml.qnode(dev, interface=\"tf\", diff_method=\"backprop\")\n",
    "def qnode(inputs, weights):\n",
    "    for i in range(blocks):\n",
    "        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "        qml.templates.StronglyEntanglingLayers(weights[i], wires=range(n_qubits)) #STRONGLY ENTANGLING LAYERS\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "weights_shape = (blocks, layers, n_qubits, 3) # Uncomment for Strongly entangling layers\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "weight_shapes = {\"weights\": weights_shape}\n",
    "inputs = tf.constant(np.random.random((batch_size, n_qubits)))\n",
    "\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "##########################-----------##########################\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "#from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#%%\n",
    "\n",
    "opt = keras.optimizers.Adam(\n",
    "        learning_rate=LR) \n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    tf.keras.backend.set_floatx(\"float32\")\n",
    "    weight_shapes = {\"weights\": weights_shape}\n",
    "    layer_1= tf.keras.layers.LSTM(512, activation='linear', input_shape=(x0.shape[1],x0.shape[2])) ### input shape: timesteps, features\n",
    "    layer_2 = tf.keras.layers.Dense(256, activation=\"linear\", dtype=tf.float32)\n",
    "    layer_3 = tf.keras.layers.Dense(128, activation=\"linear\", dtype=tf.float32)\n",
    "    layer_4 = tf.keras.layers.Dense(64, activation=\"linear\", dtype=tf.float32)\n",
    "    layer_5 = tf.keras.layers.Dense(n_qubits, activation=\"linear\", dtype=tf.float32)\n",
    "    #qlayer= tf.keras.layers.Dense(n_qubits, activation=\"linear\", dtype=tf.float32) #Classical\n",
    "    qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, n_qubits, dtype=tf.float32) \n",
    "    layer_6 = tf.keras.layers.Dense(4, activation=\"linear\", dtype=tf.float32)\n",
    "    layer_D = tf.keras.layers.Dense(1, activation=\"linear\", dtype=tf.float32)\n",
    "    \n",
    "    model1 = tf.keras.models.Sequential([layer_1,layer_2,layer_3,layer_4,layer_5,qlayer,layer_6,layer_D])\n",
    "\n",
    "#model1 = Sequential([\n",
    "#LSTM(256,return_sequences=True, input_shape=(x0.shape[1],x0.shape[2])),\n",
    "#Dense(128, input_shape=(x0.shape[1],x0.shape[2])),\n",
    "#Dense(64),\n",
    "#Dense(2),\n",
    "#qlayer,\n",
    "#Dense(1)\n",
    "#])\n",
    "    \n",
    "#Compile model\n",
    "model1.compile(loss='mean_squared_error', optimizer=opt, metrics=['mean_absolute_error',\"MAPE\",\"MAE\"])\n",
    "# Define the EarlyStopping callback\n",
    "#early_stopping = EarlyStopping(monitor='mean_absolute_error', patience=patience, verbose=1)\n",
    "#Fit model\n",
    "print(model1.summary())\n",
    "history = model1.fit(x_train_scaled, y_train, epochs=Epochs, batch_size=32\n",
    "                     ,validation_data=(x_val_scaled, y_val), shuffle=False)\n",
    "#%%\n",
    "\n",
    "# Save the training history to a file\n",
    "history_file = model_name + \"_history.txt\"\n",
    "with open(history_file, 'w') as file:\n",
    "    file.write(str(history.history))\n",
    "\n",
    "history_file = model_name + \"_Summary.txt\"\n",
    "with open(history_file, 'w') as file:\n",
    "    # Get the summary of the model architecture as a string\n",
    "    summary_string = []\n",
    "    model1.summary(print_fn=lambda x: summary_string.append(x))\n",
    "# Process the summary string into a pandas DataFrame\n",
    "    summary_data = [x.split() for x in summary_string[1:-1]]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=['Layer', 'Output','Shape', 'Param',\"type\",\"#\"])\n",
    "    file.write(str(summary_df[['Output','Param']].dropna()))\n",
    "    \n",
    "# Save the training characteristics\n",
    "#characteristics_file = model_name + \"_characteristics.txt\"\n",
    "#with open(characteristics_file, 'w') as file:\n",
    "#    file.write(\"Loss: {}\\n\".format(history.history['loss'][-1]))\n",
    "    \n",
    "#    file.write(\"Validation Loss: {}\\n\".format(history.history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a078aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = model1.predict(x_test)\n",
    "print(\"Test RMSE:\", mean_squared_error(y_test, pre, squared=False))\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, pre))\n",
    "print(\"Test MAPE:\", mean_absolute_percentage_error(y_test,pre))\n",
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test, label='Test ')\n",
    "plt.plot(pre, label='Predictions LSTM')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#%%\n",
    "plt.scatter(pre, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966821cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training characteristics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['MAPE'], label='Training MAPE')\n",
    "plt.plot(history.history['val_MAPE'], label='Validation MAPE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAPE')\n",
    "#plt.ylim(0, 200)  # Set the y-axis limits\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['MAE'], label='Training MAE')\n",
    "plt.plot(history.history['val_MAE'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "#plt.plot(np.concatenate((y_train,y_val, y_test)), label='Test ')\n",
    "plt.plot(np.array(np.concatenate((y_train,y_val, np.array(pre).flatten()))), label='QLSTM Predictions', linewidth=3, color=\"green\")\n",
    "plt.plot(np.concatenate((y_train,y_val,y_test)), label='Real', linewidth=3, color=\"blue\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(len(np.array(np.concatenate((y_train,y_val)))), ymin = -1, ymax = 1, label = \"Test set start\", linestyles = \"dashed\", linewidth=3, color=\"black\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 7))\n",
    "#plt.plot(np.concatenate((y_train,y_val, y_test)), label='Test ')\n",
    "plt.plot(np.array(np.concatenate((y_val, np.array(pre).flatten()))), label='QLSTM Predictions'\n",
    "         , linewidth=4, color=\"green\")\n",
    "plt.plot(np.concatenate((y_val,y_test)), label='Real', linewidth=4, color=\"blue\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(len((y_val)), ymin = -1, ymax = 1, label = \"Test set start\", linestyles = \"dashed\", linewidth=3, color=\"black\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling predictions if needed\n",
    "array=np.array(np.concatenate((y_val, np.array(pre).flatten())))\n",
    "A = np.array([(x - min(array)) / (max(array) - min(array)) for x in array])\n",
    "array=np.concatenate((y_val,y_test))\n",
    "B = np.array([(x - min(array)) / (max(array) - min(array)) for x in array])\n",
    "plt.figure(figsize=(16, 7))\n",
    "#plt.plot(np.concatenate((y_train,y_val, y_test)), label='Test ')\n",
    "plt.plot(A, label='QLSTM Predictions'\n",
    "         , linewidth=4, color=\"green\")\n",
    "plt.plot(B, label='Real', linewidth=4, color=\"blue\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(len((y_val)), ymin = 0, ymax = 1, label = \"Test set start\", linestyles = \"dashed\", linewidth=3, color=\"black\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d88414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plots to a file\n",
    "# Create a DataFrame from the arrays\n",
    "import pandas as pd\n",
    "df1 = pd.DataFrame()\n",
    "file_name = model_name + \"_plots.csv\"\n",
    "arr1=np.concatenate((y_train,y_val,y_test))\n",
    "arr2=np.concatenate((y_train,y_val, np.array(pre).flatten()))\n",
    "#df = pd.DataFrame({'Real': arr1,  'QLSTM': arr2})\n",
    "column_name = \"QLSTM_\" + model_name\n",
    "# Add the array to the specified column\n",
    "df1[\"Real\"] = arr1\n",
    "df1[column_name] = arr2\n",
    "# Save the DataFrame as a CSV file\n",
    "df1.to_csv(file_name, index=False)\n",
    "print(\"Arrays saved as\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e83fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
